{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name Screening\n",
    "\n",
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents <a class=\"anchor\" id=\"toc\"></a>\n",
    "\n",
    "1. [Function Definitions](#func-defs)\n",
    "    1. [Name Screening Solutions](#first-func-def)\n",
    "    2. [Plot - Execution Speed](#second-func-def)\n",
    "    3. [Plot - Number of matches](#third-func-def)\n",
    "    4. [Plot - Merged Plot](#fourth-func-def)\n",
    "    5. [Threshold Comparision for distance metric](#fifth-func-def)\n",
    "    6. [Threshold Analysis for finalizing thresholds](#sixth-func-def)\n",
    "2. [Load data](#load-data)\n",
    "3. [Data Analysis](#data-analysis)\n",
    "    1. [Generate test data](#gen-test)\n",
    "    2. [Find optimal thresholds](#find-thresholds)\n",
    "        1. [Finding metrics](#find-thresholds-part1)\n",
    "        2. [Finding Lower Limit of thresholds](#find-thresholds-part2)\n",
    "        3. [Finding Upper Limit of thresholds](#find-thresholds-part3)\n",
    "    3. [Finding Optimal Solution](#find-solution)\n",
    "4. [Generate performance plots](#gen-plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "print(\"Python Version:\", python_version())\n",
    "\n",
    "import matplotlib\n",
    "print(\"Matplotlib Version:\", matplotlib. __version__) \n",
    "\n",
    "import warnings\n",
    "#warnings.filterwarnings(action='once')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# pip install abydos\n",
    "# pip install python-Levenshtein\n",
    "\n",
    "\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from abydos import phonetic, distance\n",
    "from Levenshtein import ratio as lev_ratio\n",
    "from Levenshtein import seqratio as lev_seqratio\n",
    "from Levenshtein import setratio as lev_setratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Function Definitions <a class=\"anchor\" id=\"func-defs\"></a>\n",
    "\n",
    "Go to [Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Function Definition - All name screening solutions <a class=\"anchor\" id=\"first-func-def\"></a>\n",
    "\n",
    "Go to [Table of Contents](#toc)\n",
    "\n",
    "The function provided **FIVE** options. They are as follows:\n",
    "1. Applying Levenshtein set ratio on the names directly ***(Traditional approach)***\n",
    "2. Applying Levenshtein set ratio on the phonemes of the names\n",
    "3. Using custom Levenshtein ratio to measure similarity between the phonemes of the names\n",
    "4. Using BOTH Levenshtein set ratio on names AND custom Levenshtein distance on phonemes of the names for comparision ***(Proposed approach 1)***\n",
    "5. Using EITHER Levenshtein set ratio on names OR custom Levenshtein ratio on phonemes of the names for comparision ***(Proposed approach 2)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solutions(sol_type, db, func, thres, name, actual_name, enable_prints=True):\n",
    "    \n",
    "    print()\n",
    "    sol_name = \"\"\n",
    "    if sol_type == 1:\n",
    "        print('Solution 1 - Levenshtein set ratio on name matching')\n",
    "        sol_name = \"Baseline Solution\"\n",
    "    elif sol_type == 2:\n",
    "        print('Solution 2 - Levenshtein set ratio on Phoneme matching')\n",
    "        sol_name = \"Intermediate Solution 1\"\n",
    "    elif sol_type == 3:\n",
    "        print('Solution 3 - Custom Levenshtein Ratio on Phonemes')\n",
    "        sol_name = \"Intermediate Solution 2\"\n",
    "    elif sol_type == 4:\n",
    "        print('Solution 4 - Levenshtein set ratio AND Custom Levenshtein Ratio on Phonemes')\n",
    "        sol_name = \"Proposed Solution 1\"\n",
    "    elif sol_type == 5:\n",
    "        print('Solution 5 - Levenshtein set ratio OR Custom Levenshtein Ratio on Phonemes')\n",
    "        sol_name = \"Proposed Solution 2\"\n",
    "    else:\n",
    "        print('Invalid Option! Choose from 1 to 5')\n",
    "        return None\n",
    "    \n",
    "    print('Searched Name:', name, '\\nActual Name:', actual_name)\n",
    "    \n",
    "    results = pd.DataFrame()\n",
    "    \n",
    "    start_time= time.time()\n",
    "    \n",
    "    if sol_type != 1:\n",
    "        pn = []\n",
    "        for t in name.split():\n",
    "            pn.append(phonetic.DoubleMetaphone().encode(t)[0])\n",
    "            \n",
    "        if enable_prints:\n",
    "            print(f\"Phoneme for {name} is : {pn}\")\n",
    "    \n",
    "    for row in db.iterrows():\n",
    "        \n",
    "        if sol_type == 1:\n",
    "            metric = func(row[1]['Name'].lower(), name.lower())\n",
    "        elif sol_type == 2:\n",
    "            metric = func(' '.join(row[1]['Phonemes']).lower(), ' '.join(pn).lower())\n",
    "        elif sol_type == 3:\n",
    "            dist_score = []\n",
    "            dist_score = [max([(func(i,j)) for j in row[1]['Phonemes']]) for i in pn]\n",
    "            metric = np.mean(dist_score)\n",
    "        elif sol_type == 4 or sol_type == 5:\n",
    "            metric1 = func[0](row[1]['Name'].lower(), name.lower())\n",
    "            dist_score = []\n",
    "            dist_score = [max([(func[1](i,j)) for j in row[1]['Phonemes']]) for i in pn]\n",
    "            metric2 = np.mean(dist_score)\n",
    "        \n",
    "        if sol_type == 1 or sol_type == 2 or sol_type == 3:\n",
    "            condition = (metric >= thres)\n",
    "        elif sol_type == 4:\n",
    "            condition = (metric1 >= thres[0] and metric2 >= thres[1])\n",
    "        elif sol_type == 5:\n",
    "            condition = (metric1 >= thres[0] or metric2 >= thres[1])\n",
    "\n",
    "        \n",
    "        if condition:\n",
    "            \n",
    "            if sol_type == 1:\n",
    "                dist = np.round(lev_ratio(row[1]['Name'].lower(), name.lower()), 2)\n",
    "            elif sol_type == 2:\n",
    "                dist = np.round(lev_ratio(' '.join(row[1]['Phonemes']).lower(), ' '.join(pn).lower()), 2)\n",
    "            \n",
    "            if sol_type == 1 or sol_type == 2:\n",
    "                df2 = {'Name': row[1]['Name'], \n",
    "                       'LevSetRatio': metric, \n",
    "                       'Distance': dist}\n",
    "            elif sol_type == 3:\n",
    "                df2 = {'Name': row[1]['Name'], \n",
    "                       'Phoneme': row[1]['Phonemes'], \n",
    "                       'LevRatio': metric}\n",
    "            elif sol_type == 4 or sol_type == 5:\n",
    "                df2 = {'Name': row[1]['Name'], \n",
    "                       'Phoneme': row[1]['Phonemes'], \n",
    "                       'LevSetRatio': metric1, \n",
    "                       'LevRatio': metric2}\n",
    "\n",
    "            results = results.append(df2, ignore_index = True)\n",
    "\n",
    "    fin_time = np.round((time.time() - start_time), 2)\n",
    "    print(f\"--- Execution Time: {fin_time:,} seconds ---\")\n",
    "    \n",
    "    if sol_type == 1 or sol_type == 2:\n",
    "        if not results.empty:\n",
    "            results.sort_values('LevSetRatio', ascending=False, inplace=True)\n",
    "    elif sol_type == 3:\n",
    "        if not results.empty:\n",
    "            results.sort_values('LevRatio', ascending=False, inplace=True)\n",
    "    elif sol_type == 4 or sol_type == 5:\n",
    "        if not results.empty:\n",
    "            results.sort_values(['LevRatio', 'LevSetRatio'], ascending=False, inplace=True)\n",
    "    \n",
    "    results.reset_index(drop=True, inplace=True)\n",
    "    if enable_prints:\n",
    "        print(\"Number of matched names:\", results.shape[0])\n",
    "    \n",
    "    actual_present = False\n",
    "    if not results.empty:\n",
    "        if results[results.Name.str.contains(actual_name)].shape[0]:\n",
    "            actual_present = True\n",
    "            if enable_prints:\n",
    "                print('Results contain the actual name!')\n",
    "                display(results[results.Name.str.contains(actual_name)])\n",
    "        else:\n",
    "            actual_present = False\n",
    "            if enable_prints:\n",
    "                print('Results do not contain the actual name...')\n",
    "\n",
    "    return sol_name, results, actual_present, fin_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Function Definition - Plot horizonal bar for comparing execution speed <a class=\"anchor\" id=\"second-func-def\"></a>\n",
    "\n",
    "Go to [Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_exec_speed(results, searched_name, actual_name):\n",
    "    fig, ax = plt.subplots(figsize=(10,3))\n",
    "\n",
    "    sols = [val[0] for val in results]\n",
    "    times = [val[3] for val in results]\n",
    "    perc_change = [\" \"+str(np.round((val-times[0])*100/times[0], 2))+\" %\" for val in times]\n",
    "    bar_colors = ['green' if val[2] else 'red' for val in results]\n",
    "\n",
    "\n",
    "    ax.barh(sols, times, color=bar_colors)\n",
    "\n",
    "    for index, val in enumerate(times):\n",
    "        plt.text(val, index, val, \n",
    "                 color = 'black', horizontalalignment='left', verticalalignment='center')\n",
    "        if index == 0:\n",
    "            continue\n",
    "        plt.text(val, index, perc_change[index], \n",
    "                 color = 'white', horizontalalignment='right', verticalalignment='center')\n",
    "\n",
    "    ax.set_yticklabels(sols)\n",
    "\n",
    "    # Labels are inverted by default\n",
    "    ax.invert_yaxis()  \n",
    "\n",
    "    ax.set_xlabel('Fetch time (seconds)')\n",
    "    ax.set_title(f'Execution speed while searching for \"{actual_name}\" using \"{searched_name}\" ')\n",
    "    plt.grid(True, axis = 'x')\n",
    "\n",
    "\n",
    "    green_patch = mpatches.Patch(color='green', label='Name matched')\n",
    "    red_patch = mpatches.Patch(color='red', label='Name not matched')\n",
    "    ax.legend(handles=[green_patch, red_patch])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Function Definition - Plot merged bar and line plot to compare number of false positive matches <a class=\"anchor\" id=\"third-func-def\"></a>\n",
    "\n",
    "Go to [Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_performance_plots(df, focal_entity , limits, xlabels, figsize):\n",
    "    ax1 = sns.set_style(style=None, rc=None )\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=figsize)\n",
    "\n",
    "    g = sns.lineplot(data = df['Time'], marker='o', sort = False, ax=ax1, label=\"Execution Time\")\n",
    "    g.set(ylim=limits[0])\n",
    "    ax2 = ax1.twinx()\n",
    "    for index, row in df.iterrows():\n",
    "        g.text(row.name, \n",
    "                   row['Time'], \n",
    "                   np.round(row['Time'], 2), \n",
    "                   color='black', \n",
    "                   ha=\"center\", \n",
    "                   size='large', \n",
    "                   verticalalignment='bottom')\n",
    "\n",
    "    bplot = sns.barplot(data = df, x=focal_entity, y='Num_of_matches', alpha=0.5, ax=ax2, label=\"Number of matches\")\n",
    "    bplot.set(ylim=limits[1])\n",
    "    for index, row in df.iterrows():\n",
    "        bplot.text(row.name, \n",
    "                   row['Num_of_matches'], \n",
    "                   np.round(row['Num_of_matches'], 2), \n",
    "                   color='black', \n",
    "                   ha=\"center\", \n",
    "                   size='large', \n",
    "                   verticalalignment='baseline')\n",
    "\n",
    "    ax1.set_xticklabels(df[focal_entity], rotation=90)\n",
    "    ax1.set_xlabel(xlabels)\n",
    "    ax1.set_ylabel('Execution Time (seconds)')\n",
    "    ax2.set_ylabel('Number of matches')\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Function Definition - Plot bar plot of accuracy of each model <a class=\"anchor\" id=\"fourth-func-def\"></a>\n",
    "\n",
    "Go to [Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_TP_perc_plot(df, focal_entity, ylims, figsize):\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    g = sns.barplot(data = df, x=focal_entity, y='Name_found', alpha=0.5)\n",
    "    for index, row in df.iterrows():\n",
    "        g.text(row.name, \n",
    "                   row['Name_found'], \n",
    "                   np.round(row['Name_found'], 2), \n",
    "                   color='black', \n",
    "                   ha=\"center\", \n",
    "                   size='large', \n",
    "                   verticalalignment='baseline')\n",
    "    g.set_ylabel('Percentage of TRUE POSTIVE (%)')\n",
    "    g.set(ylim=ylims)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, axis='y')\n",
    "    plt.title('Percentage of True Positive')\n",
    "    plt.show()\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Function Definition - Threshold analysis for phonemes and names <a class=\"anchor\" id=\"fifth-func-def\"></a>\n",
    "\n",
    "Go to [Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thres_analysis1(maindb, tests):\n",
    "    \n",
    "    results = pd.DataFrame()\n",
    "    \n",
    "    for test in tests:\n",
    "        \n",
    "        actual_name = test[0]\n",
    "        \n",
    "        for name in test[1]:\n",
    "            #print()\n",
    "            #print('Searched Name:', name, '\\nActual Name:', actual_name)\n",
    "\n",
    "            pn = []\n",
    "            for t in name.split():\n",
    "                pn.append(phonetic.DoubleMetaphone().encode(t)[0])\n",
    "\n",
    "            #print(f\"Phoneme for {name} is : {pn}\")\n",
    "            \n",
    "\n",
    "            db = maindb[maindb.Name.str.contains(actual_name)]\n",
    "            for row in db.iterrows():\n",
    "\n",
    "\n",
    "                #metric1 = func[0](row[1]['Name'], name)\n",
    "                dist_score = []\n",
    "                metric0 = lev_ratio(' '.join(row[1]['Phonemes']).lower(), ' '.join(pn).lower())\n",
    "                dist_score = [max([(lev_ratio(i,j)) for j in row[1]['Phonemes']]) for i in pn]\n",
    "                metric1 = np.mean(dist_score)\n",
    "                metric2 = lev_seqratio(row[1]['Phonemes'], pn)\n",
    "                metric3 = lev_setratio(row[1]['Phonemes'], pn)\n",
    "                #print([name, , , metric2])\n",
    "\n",
    "\n",
    "                df2 = {'Name': row[1]['Name'],\n",
    "                       'Searched': name, \n",
    "                       #'Actual': actual_name, \n",
    "                       'Phoneme': row[1]['Phonemes'], \n",
    "                       'Searched Phoneme': pn, \n",
    "                       'Levenshtein Ratio': metric0, \n",
    "                       'Custom Levenshtein Ratio': metric1, \n",
    "                       'Levenshtein Seq Ratio': metric2, \n",
    "                       'Levenshtein Set Ratio': metric3}\n",
    "\n",
    "                results = results.append(df2, ignore_index = True)\n",
    "\n",
    "\n",
    "    results.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thres_analysis2(maindb, func, tests):\n",
    "    \n",
    "    results = pd.DataFrame()\n",
    "    \n",
    "    for test in tests:\n",
    "        \n",
    "        actual_name = test[0]\n",
    "        \n",
    "        for name in test[1]:\n",
    "            #print()\n",
    "            #print('Searched Name:', name, '\\nActual Name:', actual_name)\n",
    "\n",
    "            pn = []\n",
    "            for t in name.split():\n",
    "                pn.append(phonetic.DoubleMetaphone().encode(t)[0])\n",
    "\n",
    "            #print(f\"Phoneme for {name} is : {pn}\")\n",
    "            \n",
    "\n",
    "            db = maindb[maindb.Name.str.contains(actual_name)]\n",
    "            for row in db.iterrows():\n",
    "\n",
    "\n",
    "                #metric1 = func[0](row[1]['Name'].lower(), name.lower())\n",
    "                metric0 = lev_ratio(row[1]['Name'].lower(), name.lower())\n",
    "                metric1 = np.mean([max([(func[1](i,j)) for j in name.lower().split()]) for i in row[1]['Name'].lower().split()])\n",
    "                metric2 = lev_seqratio(row[1]['Name'].lower(), name.lower())\n",
    "                metric3 = lev_setratio(row[1]['Name'].lower(), name.lower())\n",
    "                dist_score = []\n",
    "                #dist_score = [max([(func[1](i,j)) for j in row[1]['Phonemes']]) for i in pn]\n",
    "                #metric2 = func[1](row[1]['Name'].lower(), name.lower())\n",
    "                #print([name, lev_seqratio(row[1]['Phonemes'], pn), lev_setratio(row[1]['Phonemes'], pn), metric2])\n",
    "\n",
    "\n",
    "                df2 = {'Name': row[1]['Name'],\n",
    "                       'Searched': name, \n",
    "                       #'Actual': actual_name, \n",
    "                       'Phoneme': row[1]['Phonemes'], \n",
    "                       'Searched Phoneme': pn, \n",
    "                       'Lev Ratio': metric0, \n",
    "                       'Custom Lev Ratio': metric1, \n",
    "                       'Lev Seq Ratio': metric2, \n",
    "                       'Lev Set Ratio': metric3}\n",
    "\n",
    "                results = results.append(df2, ignore_index = True)\n",
    "\n",
    "\n",
    "    results.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. Function Definition - Threshold finalization for phonemes and names metrics <a class=\"anchor\" id=\"sixth-func-def\"></a>\n",
    "\n",
    "Go to [Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thres_analysis(maindb, tests):\n",
    "    \n",
    "    results = pd.DataFrame()\n",
    "    \n",
    "    for test in tests:\n",
    "        \n",
    "        actual_name = test[0]\n",
    "        \n",
    "        for name in test[1]:\n",
    "            #print()\n",
    "            #print('Searched Name:', name, '\\nActual Name:', actual_name)\n",
    "\n",
    "            pn = []\n",
    "            for t in name.split():\n",
    "                pn.append(phonetic.DoubleMetaphone().encode(t)[0])\n",
    "\n",
    "            #print(f\"Phoneme for {name} is : {pn}\")\n",
    "            \n",
    "\n",
    "            db = maindb[maindb.Name.str.contains(actual_name)]\n",
    "            for row in db.iterrows():\n",
    "\n",
    "\n",
    "                #metric1 = func[0](row[1]['Name'].lower(), name.lower())\n",
    "                metric1 = lev_setratio(row[1]['Name'].lower(), name.lower())\n",
    "                metric2 = np.mean([max([(lev_ratio(i,j)) for j in name.lower().split()]) for i in row[1]['Name'].lower().split()])\n",
    "                dist_score = []\n",
    "                #dist_score = [max([(func[1](i,j)) for j in row[1]['Phonemes']]) for i in pn]\n",
    "                #metric2 = func[1](row[1]['Name'].lower(), name.lower())\n",
    "                #print([name, lev_seqratio(row[1]['Phonemes'], pn), lev_setratio(row[1]['Phonemes'], pn), metric2])\n",
    "\n",
    "\n",
    "                df2 = {'Name': row[1]['Name'],\n",
    "                       'Searched': name, \n",
    "                       #'Actual': actual_name, \n",
    "                       'Phoneme': row[1]['Phonemes'], \n",
    "                       'Searched Phoneme': pn,                         \n",
    "                       'Lev Set Ratio': metric1, \n",
    "                       'Custom Lev Ratio': metric2}\n",
    "\n",
    "                results = results.append(df2, ignore_index = True)\n",
    "\n",
    "\n",
    "    results.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thres_analysis_unknown(flag, maindb, func, tests):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    results = pd.DataFrame()\n",
    "    \n",
    "    for name in tests[-1][1]:\n",
    "        \n",
    "\n",
    "        pn = []\n",
    "        for t in name.split():\n",
    "            pn.append(phonetic.DoubleMetaphone().encode(t)[0])\n",
    "\n",
    "        db = maindb\n",
    "        print(name)\n",
    "        counter = 0\n",
    "        for row in db.iterrows():\n",
    "\n",
    "            metric1 = None\n",
    "            metric2 = None\n",
    "            if flag == 1:\n",
    "                metric1 = func[0](row[1]['Name'].lower(), name.lower())\n",
    "                condition = metric1 >=0.461\n",
    "            elif flag==2:\n",
    "                dist_score = [max([(func[1](i,j)) for j in row[1]['Phonemes']]) for i in pn]\n",
    "                metric2 = np.mean(dist_score)\n",
    "                condition = metric2 >=0.379\n",
    "                \n",
    "            if condition:\n",
    "                counter += 1\n",
    "                if counter%2500 == 0:\n",
    "                    print(counter)\n",
    "                df2 = {'Name': row[1]['Name'],\n",
    "                       'Searched': name, \n",
    "                       #'Actual': actual_name, \n",
    "                       'Phoneme': row[1]['Phonemes'], \n",
    "                       'Searched Phoneme': pn, \n",
    "                       'Lev Set Ratio': metric1, \n",
    "                       'Custom Lev Ratio': metric2}\n",
    "\n",
    "                results = results.append(df2, ignore_index = True)\n",
    "\n",
    "    print(f'Execution Time: {time.time()-start_time:,} seconds')\n",
    "    results.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Data <a class=\"anchor\" id=\"load-data\"></a>\n",
    "\n",
    "Go to [Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = pd.read_pickle('Final_Names.pkl')\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Analysis <a class=\"anchor\" id=\"data-analysis\"></a>\n",
    "\n",
    "Go to [Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Generate test data <a class=\"anchor\" id=\"gen-test\"></a>\n",
    "\n",
    "Go to [Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names.iloc[np.random.randint(names.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names[names.Name.str.contains(tests[5][0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [[\"Vladimir Nikolaevich Terentev\", [\"Vadimir Nikolevi Terente\", \n",
    "                                            \"Valdimi Terente\", \n",
    "                                            \"Baldimi Nikola Terete\", \n",
    "                                            \"Wadimi Terente\", \n",
    "                                            \"Baldimir\"]], \n",
    "         [\"Andrey Alshevskih\", [\"andre alshveck\", \n",
    "                                \"andrew alshvek\", \n",
    "                                \"andy alshevsik\", \n",
    "                                'andy alsevsikh']],\n",
    "         [\"Andrei Skoch\", ['andrew skok', \n",
    "                           'andre skokh', \n",
    "                           'andrai skosh']], \n",
    "         [\"Sheikh Aboud Rogo\", [\"Shake Abud rogo\", \n",
    "                                \"Sheik abod roguo\", \n",
    "                                'about roguo', \n",
    "                                'shaikh rogo']], \n",
    "         [\"Mohammad Fayez Al-Barsha\", [\"Mohamad Fayis Barsa\", \n",
    "                                       \"Fayes Barhsa\", \n",
    "                                       \"Mohamed Al barsha\", \n",
    "                                       'Moamad faes albasha', \n",
    "                                       'mohamad fayiz']], \n",
    "         [\"Wael Mohamad ABED AL RAHMAAN\", [\"Wel Mohammad ABID RAHMAN\", \n",
    "                                           \"Wael AL-HUSSEINI\", \n",
    "                                           \"Wel Huseni\", \n",
    "                                           \"Mohammad Rehman\"]], \n",
    "         [\"Khaled Suleiman Fayez ABOU HASSAN\", [\"Khalid Sulayman Fayiz ABU HASAN\", \n",
    "                                                \"Khaled ABU HASSAN\", \n",
    "                                                \"Kahleed Asan\", \n",
    "                                                \"Kohlid Faye Hasin\"]], \n",
    "         [\"David Amos Mazengo\", [\"Dave Masengo\", \n",
    "                                 \"Daveid Mos Masingo\", \n",
    "                                 \"Dave Masego\", \n",
    "                                 \"Daive Masego\"]], \n",
    "         [\"Shahidwror\", [\"Shahidwar\", \n",
    "                         \"Shahid war\", \n",
    "                         \"shahedvar\", \n",
    "                         \"sahaedvor\"]], \n",
    "         [\"Fares Chihabi\", [\"Faresh Chiabi\", \n",
    "                            \"Farez Chivi\", \n",
    "                            \"Faresh Jiavi\", \n",
    "                            \"Farsh Jiabi\"]], \n",
    "#         [\"Cathy Hale\", [\"Katie Hall\", \n",
    "#                         \"Kathy Hail\", \n",
    "#                         \"Kathy Hall\", \n",
    "#                         \"Cathie Hayl\", \n",
    "#                         \"Catie Hail\"]], \n",
    "#         [\"Brent Miller\", [\"Brend Miler\", \n",
    "#                           \"Brant Miller\", \n",
    "#                           \"Brend Milla\", \n",
    "#                           \"Brent Milder\", \n",
    "#                           \"Braynt Milla\"]], \n",
    "#         [\"Joseph Bolton\", [\"Josef Baultan\", \n",
    "#                            \"Josev Boltan\", \n",
    "#                            \"Joseph Baton\", \n",
    "#                            \"Josh Bolton\", \n",
    "#                            \"josaif boton\"]], \n",
    "#         [\"Mariah Cortez\", [\"Maria Cortes\", \n",
    "#                            \"Mary Gorres\", \n",
    "#                            \"Maraya Caurtes\", \n",
    "#                            \"Mariya Coddes\"]], \n",
    "         [\"DUMMY DATA\", [\"Gaurav Roy\", \n",
    "                         \"Aleh Vladimir Aziz\", \n",
    "                         \"Dipyaman Choudhury\", \n",
    "                         \"Suner Lahiri\", \n",
    "                         \"Rajendra Kumar\", \n",
    "                         \"Nishikrin Kyoji\", \n",
    "                         \"Rodrigue Patterson\", \n",
    "                         \"Anya D'Souza\", \n",
    "                         \"Joshua Matthers\", \n",
    "                         \"Michael Myers\"]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Find the optimal thresholds <a class=\"anchor\" id=\"find-thresholds\"></a>\n",
    "\n",
    "Go to [Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. Find the metrics <a class=\"anchor\" id=\"find-thresholds-part1\"></a>\n",
    "\n",
    "Go to [Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Abydos and python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abydos - Distance\n",
    "stat_time = time.time()\n",
    "for test in tests:\n",
    "    for i in test[1]:\n",
    "        (distance.dist_levenshtein(test[0], i))\n",
    "print(f'Execution Speed: {np.round((time.time()-stat_time)*1000, 4)} ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Levenshtein \n",
    "stat_time = time.time()\n",
    "for test in tests:\n",
    "    for val in test[1]:\n",
    "        (lev_ratio(test[0], val))\n",
    "print(f'Execution Speed: {np.round((time.time()-stat_time)*1000, 4)} ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc = np.arange(0,1, 0.1)\n",
    "res = thres_analysis1(names, tests)\n",
    "display(res.describe(percentiles=perc).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funcs = [[lev_ratio, lev_ratio], \n",
    "         [lev_ratio, lev_seqratio], \n",
    "         [lev_ratio, lev_setratio]]\n",
    "perc = np.arange(0,1, 0.1)\n",
    "func_names = [\"Ratio\", \"Token Sort Ratio\", \n",
    "              \"Token Set Ratio\", \"Partial Ratio\",\n",
    "              \"Partial Token Sort Ratio\", \"Partial Token Set Ratio\"]\n",
    "#sub_name = names[names.Name.str.contains(actual_name)]\n",
    "#display(sub_name)\n",
    "\n",
    "res = thres_analysis2(names, func, tests)\n",
    "display(res.describe(percentiles=perc).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results above, it is visible that the best metrics are as follows:\n",
    "- Best name comparision metric : **Levenshtein Set Ratio**\n",
    "- Best phoneme comparision metric : **Custom Levenshtein ratio**\n",
    "\n",
    "Therefore, using these to finalize the thresholds for the metric..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. Find the Lower Limit using True Positive data <a class=\"anchor\" id=\"find-thresholds-part2\"></a>\n",
    "\n",
    "Go to [Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = thres_analysis(names, tests)\n",
    "#res.sort_values(['Lev Set Ratio'])\n",
    "#res[res['Lev Set Ratio']<0.75]\n",
    "#res.sort_values(['Custom Lev Ratio'])\n",
    "#res[res['Custom Lev Ratio']<60]\n",
    "perc1 = np.arange(0,0.1, 0.05)\n",
    "perc2 = np.arange(0.1, 1, 0.1)\n",
    "perc = np.concatenate([perc1, perc2])\n",
    "display(res.describe(percentiles=perc).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = thres_analysis(names, tests)\n",
    "#res.sort_values(['Lev Set Ratio'])\n",
    "#res[res['Lev Set Ratio']<0.75]\n",
    "#res.sort_values(['Custom Lev Ratio'])\n",
    "#res[res['Custom Lev Ratio']<60]\n",
    "perc = np.arange(0.5,1, 0.05)\n",
    "display(res.describe(percentiles=perc).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the observations, to catch the variations, it is best to set the thresholds as follows:\n",
    "- Levenshtein Set Ratio = **0.461**\n",
    "- Custom Levenshtein ratio = **0.379**\n",
    "\n",
    "The next stage looks at increasing the limits, for reducing the false positives as much as possible..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3. Find the Upper Limit using False Positive data <a class=\"anchor\" id=\"find-thresholds-part3\"></a>\n",
    "\n",
    "Go to [Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "funcs = [[lev_setratio, lev_ratio]]\n",
    "\n",
    "perc = np.arange(0,1, 0.1)\n",
    "\n",
    "#sub_name = names[names.Name.str.contains(actual_name)]\n",
    "#display(sub_name)\n",
    "\n",
    "for func in funcs:\n",
    "    print(func)\n",
    "    res1 = thres_analysis_unknown(1, names, func, tests)\n",
    "    display(res1.describe(percentiles=perc).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc = np.arange(0,0.9, 0.1)\n",
    "perc2 = np.arange(0.9, 1, 0.05)\n",
    "perc = np.concatenate([perc, perc2])\n",
    "display(res1.describe(percentiles=perc).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1.sort_values('Lev Set Ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funcs = [[lev_setratio, lev_ratio]]\n",
    "\n",
    "\n",
    "perc = np.arange(0,1, 0.1)\n",
    "\n",
    "#sub_name = names[names.Name.str.contains(actual_name)]\n",
    "#display(sub_name)\n",
    "\n",
    "for func in funcs:\n",
    "    print(func)\n",
    "    res22 = thres_analysis_unknown(2, names, func, tests)\n",
    "    display(res22.describe(percentiles=perc).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc = np.arange(0,0.9, 0.1)\n",
    "perc2 = np.arange(0.9, 1, 0.05)\n",
    "perc = np.concatenate([perc, perc2])\n",
    "display(res2.describe(percentiles=perc).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2.sort_values('Custom Lev Ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc1 = np.arange(0, 0.1, 0.05)\n",
    "perc2 = np.arange(0.1,0.9, 0.1)\n",
    "perc3 = np.arange(0.9, 1, 0.05)\n",
    "perc = np.concatenate([perc1, perc2, perc3])\n",
    "display(res1.describe(percentiles=perc).T)\n",
    "display(res2.describe(percentiles=perc).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The earlier thresholds suggestions were kept as:\n",
    "- Levenshtein Set Ratio = *0.461*\n",
    "- Custom Levenshtein ratio = *0.379*\n",
    "\n",
    "However, from observating the multiple iterations, it is best to set the thresholds as follows *(keeping around the 95% percentile)*:\n",
    "- Levenshtein Set Ratio = **0.667**\n",
    "- Custom Levenshtein ratio = **0.619**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Finding best solution <a class=\"anchor\" id=\"find-solution\"></a>\n",
    "\n",
    "Go to [Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance is compared on all the test sets using the finalized metrics **Levenshtein Set Ratio & Custom Levenshtein Ratio** and using the corresponding thresholds of **0.667 & 0.619**. The different solutions are listed below:\n",
    "1. Using Levenshtein set ratio on the names directly ***(Traditional approach)***\n",
    "2. Using Levenshtein set ratio on the phonemes of the names\n",
    "3. Using custom Levenshtein ratio to measure similarity between the phonemes of the names\n",
    "4. Using BOTH Levenshtein set ratio on names AND custom Levenshtein ratio on phonemes of the names for comparision ***(Proposed approach 1)***\n",
    "5. Using EITHER Levenshtein set ratio Levenshtein set ratioon names OR custom Levenshtein ratio on phonemes of the names for comparision ***(Proposed approach 2)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lev_setratio' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5498e33ed27b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m funcs = [lev_setratio, \n\u001b[0m\u001b[0;32m      2\u001b[0m          \u001b[0mlev_setratio\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m          \u001b[0mlev_ratio\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m          \u001b[1;33m[\u001b[0m\u001b[0mlev_setratio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlev_ratio\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m          [lev_setratio, lev_ratio]]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lev_setratio' is not defined"
     ]
    }
   ],
   "source": [
    "funcs = [lev_setratio, \n",
    "         lev_setratio, \n",
    "         lev_ratio, \n",
    "         [lev_setratio, lev_ratio], \n",
    "         [lev_setratio, lev_ratio]]\n",
    "\n",
    "\n",
    "thresholds = [0.667, 0.667, 0.619, [0.667, 0.619], [0.667, 0.619]]\n",
    "\n",
    "final_results = []\n",
    "\n",
    "df = pd.DataFrame(columns={'Solution', 'Actual', 'Searched', 'Num_of_matches', 'Name_found', 'Time'})\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(len(tests[:-1])):\n",
    "    actual_name = tests[i][0]\n",
    "    display(names[names.Name.str.contains(actual_name)])\n",
    "    for j in range(len(tests[i][1])):\n",
    "        searched_name = tests[i][1][j]\n",
    "\n",
    "        for k in range(5):\n",
    "            res = solutions(k+1, names, funcs[k], thresholds[k], searched_name, actual_name, False)\n",
    "            if res is not None:\n",
    "                final_results.append(res)\n",
    "        \n",
    "        df2 = pd.DataFrame({\"Solution\": [val[0] for val in final_results], \n",
    "                            \"Actual\": actual_name, \n",
    "                            \"Searched\": searched_name, \n",
    "                            \"Num_of_matches\": [val[1].shape[0] for val in final_results],\n",
    "                            \"Name_found\": [val[2] for val in final_results], \n",
    "                            \"Time\": [val[3] for val in final_results]})\n",
    "        \n",
    "        df = pd.concat([df, df2], ignore_index=True)\n",
    "        \n",
    "        \n",
    "print(f'---- Total Execution Time: {time.time() - start_time} seconds ----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_merged_plot(df)\n",
    "df.to_pickle('Temp_save_data3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "duration = 1000  # milliseconds\n",
    "freq = 440  # Hz\n",
    "winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Generating Performance Plots <a class=\"anchor\" id=\"gen-plots\"></a>\n",
    "\n",
    "Go to [Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_pickle('Temp_save_data2.pkl')\n",
    "df = pd.read_pickle('Temp_save_data3.pkl')\n",
    "df.Num_of_matches = df.Num_of_matches.astype('int64')\n",
    "df.Name_found = df.Name_found.astype('bool')\n",
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.groupby(['Solution']).mean().reset_index()\n",
    "df2['Name_found'] = df2['Name_found']*100\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df[(df['Solution']=='Baseline Solution')].groupby(['Actual']).mean().reset_index()\n",
    "df3['Name_found'] = df3['Name_found']*100\n",
    "\n",
    "df3\n",
    "\n",
    "df4 = df[(df['Solution']=='Proposed Solution 1')].groupby(['Actual']).mean().reset_index()\n",
    "df4['Name_found'] = df4['Name_found']*100\n",
    "\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df22 = df2[~df2.Solution.str.contains('Intermediate')]\n",
    "show_performance_plots(df2, 'Solution', [(0,4),(0,1000)], \"List of solutions\", (20,5))\n",
    "show_TP_perc_plot(df2, 'Solution', (40,105), (20,5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_performance_plots(df3, 'Actual', [(0,7),(0,1000)], \"List of Actuals\", (20,5))\n",
    "show_TP_perc_plot(df3, 'Actual', (30,90), (20,5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_performance_plots(df4, 'Actual', [(0,7),(0,300)], \"List of Actuals\", (20,5))\n",
    "show_TP_perc_plot(df4, 'Actual', (30,90), (20,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
