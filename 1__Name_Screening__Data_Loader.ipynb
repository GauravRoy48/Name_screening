{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name Screening\n",
    "\n",
    "## Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents <a class=\"anchor\" id=\"toc\"></a>\n",
    "\n",
    "1. [Function Definitions - Data Loading](#func-defs)\n",
    "    1. [OFAC NS-PLC](#first-func-def)\n",
    "    2. [BIS Denied Persons](#second-func-def)\n",
    "    3. [EU FSF](#third-func-def)\n",
    "    4. [Random Names](#fourth-func-def)\n",
    "    5. [Consolidated Loader](#fifth-func-def)\n",
    "2. [Loading names from all files](#data-load)\n",
    "3. [Save the final name list](#save-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "print(\"Python Version:\", python_version())\n",
    "\n",
    "import warnings\n",
    "#warnings.filterwarnings(action='once')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# pip install abydos\n",
    "\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "from abydos import phonetic, distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Function Definitions <a class=\"anchor\" id=\"func-defs\"></a>\n",
    "\n",
    "Go to [Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Function Definition - Load OFAC NS-PLC list <a class=\"anchor\" id=\"first-func-def\"></a>\n",
    "\n",
    "Go to [Table of Contents](#toc)\n",
    "\n",
    "https://home.treasury.gov/policy-issues/financial-sanctions/consolidated-sanctions-list/non-sdn-palestinian-legislative-council-ns-plc-list\n",
    "\n",
    "Non-SDN Palestinian Legislative Council (NS-PLC) List\n",
    "\n",
    "Section (b) of General License 4 issued pursuant to the Global Terrorism Sanctions Regulations (31 C.F.R. Part 594), the Terrorism Sanctions Regulations (31 C.F.R. Part 595), and the Foreign Terrorist Organizations Sanctions Regulations (31 C.F.R. Part 597) authorizes U.S. financial institutions to reject transactions with members of the Palestinian Legislative Council (PLC) who were elected to the PLC on the party slate of Hamas, or any other Foreign Terrorist Organization (FTO), Specially Designated Terrorist (SDT), or Specially Designated Global Terrorist (SDGT), provided that any such individuals are not named on OFAC's list of Specially Designated Nationals and Blocked Persons (SDN List).\n",
    "\n",
    "In order to uniquely identify these names, OFAC has created the program code (NS-PLC). The prefix \"NS\" stands for \"non-SDN\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################\n",
    "##################          Function to check if the file is valid          #################\n",
    "#############################################################################################\n",
    "\n",
    "def load_NSPLC(fil):\n",
    "    if '.txt' in fil:\n",
    "        try:\n",
    "            with open(fil) as ifile:\n",
    "                lines = ifile.readlines()\n",
    "        except:\n",
    "            print('Error in decoding file. Exiting...')\n",
    "            return None\n",
    "        \n",
    "        intro = []\n",
    "\n",
    "        for line in lines:\n",
    "            if '__________' in line:\n",
    "                break\n",
    "            intro.append(line.strip())\n",
    "\n",
    "        intro = ' '.join(intro)\n",
    "        \n",
    "        # Keywords to validate that the file is NS-PLC list\n",
    "        ofac_strings = ['Office of Foreign Assets Control', 'OFAC', 'NS-PLC', 'non-SDN', \n",
    "                           'Palestinian Legislative Council']\n",
    "        \n",
    "        if all(x in intro for x in ofac_strings):\n",
    "            names = load_from_NSPLC(fil, lower = False, namesplit = False)\n",
    "            return names\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        print('OFAC NS-PLC list in .TXT format not found...')\n",
    "        return None\n",
    "    \n",
    "#############################################################################################\n",
    "\n",
    "\n",
    "#############################################################################################\n",
    "##################         Function to load the names from the file         #################\n",
    "#############################################################################################\n",
    "\n",
    "\n",
    "def load_from_NSPLC(filname, lower = True, namesplit = False):\n",
    "    \n",
    "    print()\n",
    "    timestamp = datetime.now().strftime(\"%d-%m-%Y, %H:%M:%S\")\n",
    "    \n",
    "    if filname.split('.')[1] != 'txt':\n",
    "        print(\"Not OFAC NS-PLC list OR list is not in recommended .txt format...\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        with open(filname) as ifile:\n",
    "            lines = ifile.readlines()\n",
    "    except:\n",
    "        print('Error in decoding file. Exiting...')\n",
    "        return None\n",
    "\n",
    "\n",
    "    print(\"OFAC NS-PLC List detected!\")\n",
    "    names = []\n",
    "    flag = 0\n",
    "    for line in lines:\n",
    "        if '___________' in line:\n",
    "            flag += 1\n",
    "            continue\n",
    "\n",
    "        if flag == 1  :\n",
    "            names.append(line.strip())\n",
    "\n",
    "\n",
    "    # Extract the names from the row of personal information\n",
    "    names1 = ' '.join(names)\n",
    "    names1 = names1.replace('  ', '\\n')\n",
    "    names1 = names1.split('\\n')\n",
    "    names1 = [name.split('DOB')[0] for name in names1]\n",
    "\n",
    "\n",
    "    # Split the name aliases\n",
    "    names2 = []\n",
    "    for name in names1:\n",
    "        if \"(a.k.a\" in name:\n",
    "            names2.append(name.split(')')[0])\n",
    "        elif \" a.k.a\" in name:\n",
    "            names2.append(name.split('DOB')[0])\n",
    "        else:\n",
    "            names2.append(','.join(name.replace(';', ',').split(',')[:2]))\n",
    "\n",
    "    # Replace letters in names that do not use anglosized letters\n",
    "    names3 = []\n",
    "    for name in names2:\n",
    "        nms = name.strip().split('a.k.a.')\n",
    "        new_nms = []\n",
    "        for nm in nms:\n",
    "            # a-z, A-Z, '-', ' ', ',' included\n",
    "            nm = re.sub('[^a-zA-ZÀ-ÿ- ,]+', '', nm).strip()\n",
    "            new_nms.append(nm)\n",
    "        names3.append(new_nms)\n",
    "        #names3.append(name.strip().split('a.k.a.'))\n",
    "\n",
    "    \n",
    "    \n",
    "    if lower:\n",
    "        names3 = [[n.lower() for n in nm] for nm in names3]\n",
    "    \n",
    "    # All names are provided as (Last-Name, First-Name). The chunk rearranges them back to (First-Name Last-Name)\n",
    "    if not namesplit:\n",
    "        new_names = []\n",
    "        for name in names3:\n",
    "            nn = []\n",
    "            for n in name:\n",
    "                #print(' '.join(n.split(',')[::-1]))\n",
    "                nn.append(' '.join(n.split(',')[::-1]).strip())\n",
    "            new_names.append(nn)\n",
    "        \n",
    "        names3 = new_names\n",
    "    \n",
    "    print(f\"{len(names3):,} names detected!\")\n",
    "    \n",
    "    # Remove duplicate names\n",
    "    new_names = []\n",
    "    for name in names3:\n",
    "        flag = True\n",
    "        for nn in new_names:\n",
    "            if sorted(name) == sorted(nn):\n",
    "                flag = False\n",
    "                break\n",
    "            \n",
    "        if flag:\n",
    "            new_names.append(name)\n",
    "        flag = True\n",
    "            \n",
    "    names3 = new_names\n",
    "    \n",
    "    print(f\"{len(names3):,} unique names found!\")\n",
    "        \n",
    "    \n",
    "    # Get final list of names that are usable and non-usable based on letters in names\n",
    "    pho_list = []\n",
    "    fin_names = []\n",
    "    exc_list = []\n",
    "    for nam in names3:\n",
    "        pho = []\n",
    "        max_len = 0\n",
    "        max_nam = \"\"\n",
    "        for n in nam:\n",
    "            if len(n.split()) > max_len:\n",
    "                max_len = len(n.split())\n",
    "                max_nam = n\n",
    "                \n",
    "            n1 = n.replace('-', ' ').replace('.', ' ')\n",
    "            for sn in n1.split():\n",
    "                pho.append(phonetic.DoubleMetaphone().encode(sn)[0])\n",
    "        \n",
    "        if not re.match(\"^[A-Za-z0-9À-ÿ -.()/]*$\", max_nam):\n",
    "            exc_list.append(nam)\n",
    "            continue\n",
    "        fin_names.append(max_nam)\n",
    "        pho = list(dict.fromkeys(pho))\n",
    "        pho_list.append(pho)\n",
    "        \n",
    "    \n",
    "    print(f\"{len(fin_names):,} unique anglosized names extracted!\")\n",
    "\n",
    "    df = pd.DataFrame(data={\"List\": \"NS-PLC\", \"Name\": fin_names, \"Phonemes\": pho_list, \"Timestamp\": timestamp})\n",
    "    exc_df = pd.DataFrame(data={\"List\": \"NS-PLC\", \"Name\": exc_list, \"Reason\": \"Non-anglosized letters in name\", \"Timestamp\": timestamp})\n",
    "    \n",
    "    return df, exc_df\n",
    "\n",
    "########################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Function Definition - Load Bureau of Industry and Security: Denied Persons List <a class=\"anchor\" id=\"second-func-def\"></a>\n",
    "\n",
    "Go to [Table of Contents](#toc)\n",
    "\n",
    "https://www.bis.doc.gov/index.php/policy-guidance/lists-of-parties-of-concern/denied-persons-list\n",
    "\n",
    "\n",
    "The Denied Persons List is a list of people and companies whose export privileges have been denied by the Department of Commerce's Bureau of Industry and Security (BIS). An American company or individual may not participate in an export transaction with an individual or company on the Denied Persons List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#############################################################################################\n",
    "##################          Function to check if the file is valid          #################\n",
    "#############################################################################################\n",
    "\n",
    "def load_DPL(fil):\n",
    "    if '.txt' in fil:\n",
    "        try:\n",
    "            with open(fil) as ifile:\n",
    "                lines = ifile.readlines()\n",
    "        except:\n",
    "            print('Error in decoding file. Exiting...')\n",
    "            return None\n",
    "        \n",
    "        intro = []\n",
    "\n",
    "        for line in lines:\n",
    "            if '__________' in line:\n",
    "                break\n",
    "            intro.append(line.strip())\n",
    "\n",
    "        intro = ' '.join(intro)\n",
    "        \n",
    "        # keywords that validate that the file is the Denied Persons list\n",
    "        bis_strings = [\"Name\", \"Street_Address\", \"City\", \"State\", \"Country\", \"Postal_Code\",\n",
    "                            \"Effective_Date\", \"Expiration_Date\", \"Standard_Order\", \"Last_Update\", \n",
    "                            \"Action\", \"FR_Citation\"]\n",
    "        \n",
    "        if all(x in intro for x in bis_strings):\n",
    "            names = load_from_BIS_Denied_Persons(fil, lower=False)\n",
    "            return names\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    else:\n",
    "        print('BIS Denied Persons list in .TXT format not found...')\n",
    "        return None\n",
    "\n",
    "#############################################################################################\n",
    "\n",
    "\n",
    "#############################################################################################\n",
    "##################         Function to load the names from the file         #################\n",
    "#############################################################################################\n",
    "\n",
    "\n",
    "def load_from_BIS_Denied_Persons(filname, lower=True):\n",
    "    \n",
    "    print()\n",
    "    timestamp = datetime.now().strftime(\"%d-%m-%Y, %H:%M:%S\")\n",
    "    \n",
    "    # Remove words that are not associated with human names\n",
    "    stopwords = ['advanced', 'airlines', 'technology', 'ltd', 'limited', 'corporation', 'corp', 'llc', 'trade', \n",
    "                 'international', 'trading', 'globe', 'systems', 'products', 'computers', 'inc', '.net', 'blue bird', \n",
    "                 'group', 'aviation', 'gmbh', 'enterprises', 'results', 'commercio', 'engineering']\n",
    "    \n",
    "    if filname.split('.')[1] != 'txt':\n",
    "        print(\"Not Bureau of Industry and Security Denied Persons List OR list is not in recommended .txt format...\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        db = pd.read_csv(filname, delimiter='\\t')\n",
    "    except:\n",
    "        print('Error in decoding file. Exiting...')\n",
    "        return None\n",
    "\n",
    "    print(\"BIS Denied Persons List detected!\")\n",
    "    \n",
    "    print(f\"{len(db):,} names detected!\")\n",
    "    \n",
    "    db.drop_duplicates(subset='Name', inplace=True)\n",
    "    \n",
    "    print(f\"{len(db):,} unique names found!\")\n",
    "    \n",
    "    names = db['Name'].tolist()\n",
    "    if lower:\n",
    "        names = [n.lower() for n in names]\n",
    "    \n",
    "    # Choose person names instead of companies, corporations, etc.\n",
    "    nn = []\n",
    "    exc_list = []\n",
    "    for name in names:\n",
    "        if not any(x in name.lower() for x in stopwords):\n",
    "            nn.append(name)\n",
    "        else:\n",
    "            exc_list.append(name)\n",
    "    \n",
    "    exc_df = pd.DataFrame({\"List\": \"BIS Denied Persons List\", \"Name\": exc_list, \"Reason\": \"Not a person's name\", \"Timestamp\": timestamp})\n",
    "    \n",
    "    names = nn\n",
    "    print(f\"{len(names):,} unique persons names found!\")\n",
    "    \n",
    "    # Choose names that make use of english alphabets\n",
    "    pho_list = []\n",
    "    fin_names = []\n",
    "    for nam in names:\n",
    "        pho = []\n",
    "        if not re.match(\"^[A-Za-z0-9À-ÿ -.()/]*$\", nam):\n",
    "            exc_df.loc[len(exc_df.index)] = [\"BIS Denied Persons List\", nam, \"Non-anglosized letters in name\", timestamp]\n",
    "            continue\n",
    "        \n",
    "        nam1 = nam.replace('-', ' ').replace('.', ' ')\n",
    "        for sn in nam1.split():\n",
    "            pho.append(phonetic.DoubleMetaphone().encode(sn)[0])\n",
    "        \n",
    "        \n",
    "        fin_names.append(nam)\n",
    "        pho = list(dict.fromkeys(pho))\n",
    "        pho_list.append(pho)\n",
    "        \n",
    "    \n",
    "    print(f\"{len(fin_names):,} unique anglosized names extracted!\")\n",
    "\n",
    "    df = pd.DataFrame(data={\"List\": \"BIS Denied Persons List\", \"Name\": fin_names, \"Phonemes\": pho_list, \"Timestamp\": timestamp})\n",
    "    \n",
    "    return df, exc_df\n",
    "\n",
    "################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Function Definition - European Union: Financial Sanctions List <a class=\"anchor\" id=\"third-func-def\"></a>\n",
    "\n",
    "Go to [Table of Contents](#toc)\n",
    "\n",
    "https://eeas.europa.eu/headquarters/headquarters-homepage_en/8442/Consolidated%20list%20of%20sanctions\n",
    "\n",
    "In order to facilitate the application of financial sanctions, the European Banking Federation, the European Savings Banks Group, the European Association of Co-operative Banks and the European Association of Public Banks (\"the EU Credit Sector Federations\") and the Commission recognised the need for an EU consolidated list of persons, groups and entities subject to CFSP related financial sanctions. It was therefore agreed that the Credit Sector Federations would set up a database containing the consolidated list for the Commission, which would host and maintain the database and keep it up-to-date. This database was developed first and foremost to assist the members of the EU Credit Sector Federations in their compliance with financial sanctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#############################################################################################\n",
    "##################          Function to check if the file is valid          #################\n",
    "#############################################################################################\n",
    "\n",
    "def load_FSF(fil):\n",
    "    if '.csv' in fil:\n",
    "        \n",
    "        # Keywords to validate the FSF list\n",
    "        fsf_strings = ['NameAlias_WholeName', 'Naal_wholename']\n",
    "        \n",
    "        try:\n",
    "            db = pd.read_csv(fil, delimiter=';')\n",
    "        except:\n",
    "            print('Error in decoding file. Exiting...')\n",
    "            return None\n",
    "\n",
    "        if len([i for i in fsf_strings if i in db.columns]) > 0:\n",
    "            names = load_from_EU_FSF(fil)\n",
    "            return names\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    else:\n",
    "        print('EU FSF list in .CSV format not found...')\n",
    "        return None\n",
    "    \n",
    "#############################################################################################\n",
    "\n",
    "\n",
    "#############################################################################################\n",
    "##################         Function to load the names from the file         #################\n",
    "#############################################################################################\n",
    "\n",
    "\n",
    "def load_from_EU_FSF(filname):\n",
    "    \n",
    "    print()\n",
    "    timestamp = datetime.now().strftime(\"%d-%m-%Y, %H:%M:%S\")\n",
    "    \n",
    "    if filname.split('.')[1] != 'csv':\n",
    "        print(\"Not EU FSF Consolidated list OR list is not in recommended .csv format...\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        db = pd.read_csv(filname, delimiter=';')\n",
    "    except:\n",
    "        print('Error in decoding file. Exiting...')\n",
    "        return None\n",
    "    \n",
    "    print(\"EU-FSF List detected!\")\n",
    "    \n",
    "    # There are 2 versions of FSF lists; the condition checks which version is available\n",
    "    names = []\n",
    "    if 'NameAlias_WholeName' in db.columns:\n",
    "        namecol = 'NameAlias_WholeName'\n",
    "        db.dropna(subset=[namecol], inplace=True)\n",
    "        db.drop_duplicates(subset=namecol, inplace=True)\n",
    "        print(f\"{len(db):,} names detected!\")\n",
    "        names = db[db['Entity_SubjectType']=='P'][namecol].tolist()\n",
    "        \n",
    "    elif 'Naal_wholename' in db.columns:\n",
    "        namecol = 'Naal_wholename'\n",
    "        db.dropna(subset=[namecol], inplace=True)\n",
    "        db.drop_duplicates(subset=namecol, inplace=True)\n",
    "        print(f\"{len(db):,} names detected!\")\n",
    "        names = db[db['Subject_type']=='P'][namecol].tolist()\n",
    "    \n",
    "    names = [name.lower() for name in names]\n",
    "    names = list(set(names))\n",
    "    print(f\"{len(names):,} unique persons names found!\")\n",
    "        \n",
    "    # Select names that use english alphabets\n",
    "    pho_list = []\n",
    "    fin_names = []\n",
    "    exc_list = []\n",
    "    for nam in names:\n",
    "        pho = []\n",
    "        if not re.match(\"^[A-Za-z0-9À-ÿ -.()/]*$\", nam):\n",
    "            exc_list.append(nam)\n",
    "            continue\n",
    "        \n",
    "        nam1 = nam.replace('-', ' ').replace('.', ' ')\n",
    "        for sn in nam1.split():\n",
    "            pho.append(phonetic.DoubleMetaphone().encode(sn)[0])\n",
    "        \n",
    "        \n",
    "        fin_names.append(nam.title())\n",
    "        pho = list(dict.fromkeys(pho))\n",
    "        pho_list.append(pho)\n",
    "        \n",
    "    \n",
    "    print(f\"{len(fin_names):,} unique anglosized names extracted!\")\n",
    "\n",
    "    df = pd.DataFrame(data={\"List\": \"EU-FSF\", \"Name\": fin_names, \"Phonemes\": pho_list, \"Timestamp\": timestamp})\n",
    "    exc_df = pd.DataFrame(data={\"List\": \"EU-FSF\", \"Name\": exc_list, \"Reason\": \"Non-anglosized letters in name\", \"Timestamp\": timestamp})\n",
    "    \n",
    "    return df, exc_df\n",
    "\n",
    "#############################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = \"hello-ther.ee you. \"\n",
    "\n",
    "val = val.replace('-', ' ').replace('.', ' ')\n",
    "\n",
    "val.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Function Definition - Random Name List <a class=\"anchor\" id=\"fourth-func-def\"></a>\n",
    "\n",
    "Go to [Table of Contents](#toc)\n",
    "\n",
    "https://fossbytes.com/tools/random-name-generator\n",
    "\n",
    "An online tool (link provided) has been used to generate around 100,000 English, Spanish, Italian and German names to increase the size of the final name dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#############################################################################################\n",
    "##################             Function to load the random names            #################\n",
    "#############################################################################################\n",
    "\n",
    "\n",
    "def load_random():\n",
    "    # 20,000 random names generated using https://fossbytes.com/tools/random-name-generator\n",
    "    \n",
    "    print()\n",
    "    timestamp = datetime.now().strftime(\"%d-%m-%Y, %H:%M:%S\")\n",
    "    \n",
    "    try:\n",
    "        files = ['random_names_fossbytes_english.csv',\n",
    "                 'random_names_fossbytes_spanish.csv',\n",
    "                 'random_names_fossbytes_italian.csv',\n",
    "                 'random_names_fossbytes_german.csv']\n",
    "        \n",
    "        dbs = []\n",
    "        for filename in files:\n",
    "            n = pd.read_csv(filename, header=None).shape[0]\n",
    "            s = 5000\n",
    "            skip = sorted(random.sample(range(1,n+1),n-s)) #the 0-indexed header will not be included in the skip list\n",
    "            \n",
    "            df = pd.read_csv(filename, header=None, skiprows=skip)\n",
    "            dbs.append(df)\n",
    "\n",
    "        db = pd.concat(dbs, ignore_index=True, sort=False)\n",
    "    except:\n",
    "        print('Error in decoding file. Exiting...')\n",
    "        return None\n",
    "        \n",
    "    print(\"Random name list loaded!\")\n",
    "    \n",
    "    print(f\"{db.shape[0]:,} names detected!\")\n",
    "    names = db[0].tolist()\n",
    "    names = list(set(names))\n",
    "    print(f\"{len(names):,} unique persons names found!\")\n",
    "        \n",
    "    \n",
    "    pho_list = []\n",
    "    fin_names = []\n",
    "    exc_list = []\n",
    "    for nam in names:\n",
    "        pho = []\n",
    "        if not re.match(\"^[A-Za-z0-9À-ÿ -.()/]*$\", nam):\n",
    "            exc_list.append(nam)\n",
    "            continue\n",
    "        \n",
    "        nam1 = nam.replace('-', ' ').replace('.', ' ')\n",
    "        for sn in nam1.split():\n",
    "            pho.append(phonetic.DoubleMetaphone().encode(sn)[0])\n",
    "        \n",
    "        \n",
    "        fin_names.append(nam.title())\n",
    "        pho = list(dict.fromkeys(pho))\n",
    "        pho_list.append(pho)\n",
    "        \n",
    "    \n",
    "    print(f\"{len(fin_names):,} unique anglosized names extracted!\")\n",
    "\n",
    "    df = pd.DataFrame(data={\"List\": \"RandomGen\", \"Name\": fin_names, \"Phonemes\": pho_list, \"Timestamp\": timestamp})\n",
    "    exc_df = pd.DataFrame(data={\"List\": \"RandomGen\", \"Name\": exc_list, \"Reason\": \"Non-anglosized letters in name\", \"Timestamp\": timestamp})\n",
    "    \n",
    "    return df, exc_df\n",
    "\n",
    "#############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Function Definition - Load Files <a class=\"anchor\" id=\"fifth-func-def\"></a>\n",
    "\n",
    "Go to [Table of Contents](#toc)\n",
    "\n",
    "Consolidation function that allows for loading of the different name lists with appropriate checks in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#############################################################################################\n",
    "##################      Function to load the files based on user input      #################\n",
    "#############################################################################################\n",
    "\n",
    "def load_files(disable_random=False):\n",
    "    \n",
    "    print('''Hello user! This script allows you to load 3 types of restricted persons files:\n",
    "            1. OFAC NS-PLC list (in TXT format)\n",
    "            2. BIS Denied Persons List (in TXT format)\n",
    "            3. EU FSF list (in CSV format)''')\n",
    "    ftype = input('''Following are the instructions to load the files from the current path (please make sure that the files exist in the same path as this script):\n",
    "        1. (Default) Type 'all' for loading all the files in the current path\n",
    "        2. Type 'ofac' for loading the OFAC NS-PLC list in TXT format\n",
    "        3. Type 'dpl' for loading the Bureau of Industry and Security - Denied Persons List in TXT format\n",
    "        4. Type 'fsf' for loading the EU Sanction \n",
    "        \n",
    "        Choice: ''')\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    if ftype == '':\n",
    "        print('\\tNo choice entered. Default choice \\'all\\' selected!')\n",
    "        ftype='all'\n",
    "    \n",
    "    global_names = pd.DataFrame()\n",
    "    global_exc_names = pd.DataFrame()\n",
    "    \n",
    "    if ftype == 'all':\n",
    "        relev_fil = [fil for fil in os.listdir() if '.csv' in fil or '.txt' in fil]\n",
    "        \n",
    "        for fil in relev_fil:\n",
    "            if '.txt' in fil:\n",
    "                names = load_NSPLC(fil)\n",
    "                if names is not None:\n",
    "                    global_names = global_names.append(names[0], ignore_index=True)\n",
    "                    global_exc_names = global_exc_names.append(names[1], ignore_index=True)\n",
    "                    \n",
    "                names = load_DPL(fil)\n",
    "                if names is not None:\n",
    "                    global_names = global_names.append(names[0], ignore_index=True)\n",
    "                    global_exc_names = global_exc_names.append(names[1], ignore_index=True)\n",
    "                    \n",
    "            elif '.csv' in fil:\n",
    "                names = load_FSF(fil)\n",
    "                if names is not None:\n",
    "                    global_names = global_names.append(names[0], ignore_index=True)\n",
    "                    global_exc_names = global_exc_names.append(names[1], ignore_index=True)\n",
    "                    \n",
    "    \n",
    "    elif ftype == 'ofac':\n",
    "        relev_fil = [fil for fil in os.listdir() if '.txt' in fil]\n",
    "        \n",
    "        for fil in relev_fil:\n",
    "            if '.txt' in fil:\n",
    "                names = load_NSPLC(fil)\n",
    "                if names is not None:\n",
    "                    global_names = global_names.append(names[0], ignore_index=True)\n",
    "                    global_exc_names = global_exc_names.append(names[1], ignore_index=True)\n",
    "    \n",
    "    elif ftype == 'dpl':\n",
    "        relev_fil = [fil for fil in os.listdir() if '.txt' in fil]\n",
    "        \n",
    "        for fil in relev_fil:\n",
    "            if '.txt' in fil:\n",
    "                names = load_DPL(fil)\n",
    "                if names is not None:\n",
    "                    global_names = global_names.append(names[0], ignore_index=True)\n",
    "                    global_exc_names = global_exc_names.append(names[1], ignore_index=True)\n",
    "\n",
    "    elif ftype == 'fsf':\n",
    "        relev_fil = [fil for fil in os.listdir() if '.csv' in fil]\n",
    "        \n",
    "        for fil in relev_fil:\n",
    "            if '.csv' in fil:\n",
    "                names = load_FSF(fil)\n",
    "                if names is not None:\n",
    "                    global_names = global_names.append(names[0], ignore_index=True)\n",
    "                    global_exc_names = global_exc_names.append(names[1], ignore_index=True)\n",
    "        \n",
    "    if not disable_random:\n",
    "        names = load_random()\n",
    "        if names is not None:\n",
    "            global_names = global_names.append(names[0], ignore_index=True)\n",
    "            global_exc_names = global_exc_names.append(names[1], ignore_index=True)\n",
    "    \n",
    "    \n",
    "    print(f\"\\n--- Execution Time: {np.round((time.time() - start_time)*1000, 2):,} ms ---\")\n",
    "    inc_shape = global_names.shape[0]\n",
    "    exc_shape = global_exc_names.shape[0]\n",
    "    print(f'\\n\\nTotal {inc_shape:,} names fetched!')\n",
    "    print(f'\\n\\nTotal {exc_shape:,} ({np.round(exc_shape*100/(inc_shape+exc_shape), 2)} %) names excluded!')\n",
    "    return global_names, global_exc_names\n",
    "\n",
    "#############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading names <a class=\"anchor\" id=\"data-load\"></a>\n",
    "\n",
    "Go to [Table of Contents](#toc)\n",
    "\n",
    "This script allows you to load 3 types of restricted persons files:\n",
    "1. OFAC NS-PLC list (in TXT format)\n",
    "2. BIS Denied Persons List (in TXT format)\n",
    "3. EU FSF list (in CSV format)\n",
    "\n",
    "To select the appropriate files, type one of the options, when prompted:\n",
    "- all - (Default) Type 'all' for loading all the files in the current path\n",
    "- ofac - Type 'ofac' for loading the OFAC NS-PLC list in TXT format\n",
    "- dpl - Type 'dpl' for loading the Bureau of Industry and Security - Denied Persons List in TXT format\n",
    "- fsf - Type 'fsf' for loading the EU Sanction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_random = True # ignores random names\n",
    "#disable_random = False # includes random names\n",
    "\n",
    "\n",
    "names, exc = load_files(disable_random=disable_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final list of acceptable names\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names.groupby('List').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final list of rejected names with reasons\n",
    "exc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Save the final name list <a class=\"anchor\" id=\"save-data\"></a>\n",
    "\n",
    "Go to [Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if disable_random:\n",
    "    ### SAVE NAMES without RANDOM when generated\n",
    "    names.to_pickle('Final_Names_wo_Random.pkl')\n",
    "    exc.to_pickle('Excluded_Names_wo_Random.pkl')\n",
    "else:\n",
    "    ### SAVE NAMES with RANDOM when generated\n",
    "    names.to_pickle('Final_Names_w_Random.pkl')\n",
    "    exc.to_pickle('Excluded_Names_w_Random.pkl')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
